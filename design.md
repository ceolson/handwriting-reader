Inspired by Philipp Kühand and Daniel Kirsch’s Detextify app, for our final project we created a handwriting recognizer. The goal of our project was to create a web app that would be able to determine a user inputted number (via drawing on the website or by uploading a picture of the number). We started our project by writing a neural network from scratch; however, we quickly realized that this would be a project that would quickly exceed the maximum scope of time we had. As a result, we decided to use Keras to implement our project. Keras is a “a high-level neural networks API” that is written in python. We ran Keras on top of TensorFlow, an open source software library commonly used for machine learning. 

Once we successfully installed TensorFlow, Keras, Flask, and related dependencies on our local machine’s virtual environment, we trained our network with the Modified National Institute of Standards and Technology (MNIST) dataset. The MNIST database contains a training set of 60,000 handwritten digits and a test set of 10,000 images. Our first major task was to download and process the entire MNIST dataset into a format that expected by the Keras network. We knew each image was 28x28 pixels, so after determining the offset of the data, we appended each pixel for each image into a one dimensional array. We also processed the labels, or actual values, for each image, and appended these to the a corresponding array. These two arrays were then returned as a tuple to our train.py function. 

We chose to train our network with 3 layers (as you can see in lines 16-18 of `train.py`). After experimenting with different sets of training models, we found that increasing the number of layers did not have notable impact on our networks ability to correctly recognize digits. However, since we are able to save our model after training, luckily training time does not play a factor in our project’s final product. We also chose to use ‘relu’ as our activation function for layers 1 and 2 and ‘softmax’ for layer 3. Simply put, activation functions decide whether the neuron should be fired, or ‘activated’ or not, based on the output of each neuron’s weighted sum of inputs plus bias. The relu function is a good choice of activation function because, it relatively computationally cheap: the gradient computation just involves assigning either 0 or 1 depending on the sign, and the computational step simply sets negative elements to 0.0. In addition, in comparison to a sigmoid function, the relu activation allows for multiple layers that may be useful for as the network grows. Finally, we determined out how to save our trained model (line 39 train.py) so that we would not have to re-run this time consuming process every time a user submitted a digit to be identified. 

After our model was finally trained, we were able to test it with user-drawn input on the website. However; we quickly realized that our model was still quite inaccurate. In order to increase accuracy, we decided to pre-process the user-drawn images so that they would look more similar to the database’s expected images. For user-drawn inputs we realized that the binary contrast between drawn and empty space (black strokes correspond to 1.0 and white space corresponds to 0.0 on our sudo-bitmap) did not look similar to the life-like grayscale the MNIST dataset was expecting. As a result, we decided to increase the width of a single stroke to 2 pixels. From here we applied the blur function (located in `vector_helpers.py`) to create a gradient of 0.5 surrounding the drawn image. In order to see the effects of our blurring, we created the read template (useful for debugging but not visible in the final product).Similarly, once we added the ability for user’s to upload their own images, we had to figure out a way to standardize these images so that they can also be recognized by our network. We decided to add several key functions located in `bitmap_helpers.py` including squareify and scale_to functions, which ensure the drawn image was scaled to 28 by 28 pixels (like the MNIST dataset), and gray, which ensures all images are standardized on a grayscale (supposing a user inputs uploads a digit drawn in red ink etc). Combined, these efforts greatly increased our network's ability to correctly identify the user-inputted images.

Lastly, we decided that we want our network to grow; we decided to make our neural network dynamic as we modified it to learn from the user inputs as well. In our front-end design, we added an option for users to mark whether our guess was correct. If not, we request users write in the correct label for their number. From here, we append the new user inputted image and correct label to our existing neural network and re-train. While it will take a significant number of images to actually make an impact on the network (since the database already has 60,000 images), we noticed that after several ‘re-trainings’ with the same uploaded images, our network was able to successfully recognize the image where the initial guess was incorrect. 

In the future, we hope to continue improving our network by exploring more options for image pre-processing. One idea is to somehow center our user drawn-images, as the MNIST database is all centered. In this vein, we could also potentially retrain our network with a larger dataset, which would include our version of a randomly offset, off-center, MNIST database.